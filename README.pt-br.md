<div align="center">

# üß† SenseiDB - O Agente de IA Pessoal com RAG
### Um mentor de IA privado e contextual que se adapta ao seu conhecimento.

![Python](https://img.shields.io/badge/Python-3.11-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Django](https://img.shields.io/badge/Django-API-092E20?style=for-the-badge&logo=django&logoColor=white)
![React](https://img.shields.io/badge/React-Frontend-61DAFB?style=for-the-badge&logo=react&logoColor=black)
![GCP](https://img.shields.io/badge/Google_Cloud-Run-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

</div>

---

## üöÄ Conceito Central

O **SenseiDB** √© um assistente de IA pessoal projetado para ser um mentor estrat√©gico que se adapta √† base de conhecimento privada do usu√°rio. Ele utiliza uma arquitetura **RAG (Retrieval-Augmented Generation)** para fornecer respostas contextualmente relevantes com base em seus documentos, anota√ß√µes e dados.

Diferente de chatbots gen√©ricos, o SenseiDB baseia suas respostas em suas informa√ß√µes fornecidas, garantindo privacidade e entregando insights personalizados e precisos. √â uma IA que realmente conhece o seu mundo.

---

## üèõÔ∏è Fluxo da Arquitetura RAG

O sistema √© projetado em torno do padr√£o RAG para garantir que as respostas da IA sejam aumentadas com contexto factual fornecido pelo usu√°rio.

```mermaid
graph TD
    subgraph "Interface do Usu√°rio (React)"
        UI(Interface do Usu√°rio)
    end

    subgraph "Servi√ßos de Backend (Django no GCP Cloud Run)"
        API(API Gateway)
        EmbeddingService(Servi√ßo de Embedding)
        VectorDB[(Banco de Dados Vetorial)]
        LLM_Service(Servi√ßo de LLM<br/>Gemini / Groq)
    end

    subgraph "Externo"
        LLM_API(API do Provedor de LLM)
    end

    UI -- 1. "Como X funciona?" --> API
    API -- 2. "Como X funciona?" --> EmbeddingService
    EmbeddingService -- 3. [Vetor de Embedding] --> VectorDB
    VectorDB -- 4. Retorna Peda√ßos Relevantes --> API
    API -- 5. "Contexto: [Peda√ßos]<br/>Pergunta: Como X funciona?" --> LLM_Service
    LLM_Service -- 6. Encaminha Prompt --> LLM_API
    LLM_API -- 7. Resposta Gerada --> LLM_Service
    LLM_Service -- 8. Resposta Formatada --> API
    API -- 9. Resposta Final --> UI

    style VectorDB fill:#4285F4,stroke:#fff,stroke-width:2px,color:#fff
    style LLM_Service fill:#34A853,stroke:#fff,stroke-width:2px,color:#fff
```

1.  **Pergunta:** O usu√°rio faz uma pergunta atrav√©s do frontend em React.
2.  **Embedding:** O backend Django gera uma representa√ß√£o vetorial (embedding) da pergunta.
3.  **Recupera√ß√£o:** O sistema consulta um banco de dados vetorial para encontrar os "peda√ßos" de texto mais relevantes dos documentos carregados pelo usu√°rio.
4.  **Aumento:** Os peda√ßos recuperados s√£o anexados √† pergunta original, criando um prompt aumentado.
5.  **Gera√ß√£o:** Este prompt aumentado √© enviado para um Grande Modelo de Linguagem (LLM) poderoso como o Gemini do Google ou o Llama do Groq.
6.  **Resposta:** O LLM gera uma resposta factual e ciente do contexto, que √© enviada de volta ao usu√°rio.

---

## ‚ú® Funcionalidades Chave

-   **Base de Conhecimento Privada:** Carregue seus documentos (PDF, TXT, etc.) para criar um contexto seguro e privado para a IA.
-   **Q&A Contextual:** Fa√ßa perguntas complexas e receba respostas baseadas em seus dados, completas com fontes.
-   **Suporte a M√∫ltiplos LLMs:** Alterna dinamicamente entre provedores de LLM como Google AI (Gemini) e Groq para otimiza√ß√£o de custo/velocidade e redund√¢ncia.
-   **Backend RESTful:** Uma API Django robusta gerencia toda a l√≥gica de neg√≥cio, autentica√ß√£o de usu√°rio e orquestra√ß√£o da IA.
-   **Ambiente Containerizado:** Totalmente containerizado com Docker para paridade entre desenvolvimento e produ√ß√£o, garantindo que "funciona na minha m√°quina" signifique que funciona na nuvem.

---

## ‚öôÔ∏è Stack de Tecnologias

| Camada | Tecnologia | Prop√≥sito |
| :--- | :--- | :--- |
| **Frontend** | React | Uma Single Page Application (SPA) responsiva e moderna para chat e gerenciamento de documentos. |
| **Backend** | Django, Django Rest Framework | API RESTful segura e escal√°vel para l√≥gica de neg√≥cio e orquestra√ß√£o de IA. |
| **IA/ML** | Google AI (Gemini), Groq (Llama) | M√∫ltiplos provedores de Grandes Modelos de Linguagem para gera√ß√£o de texto. |
| **Banco de Dados** | Banco de Dados Vetorial (ex: Pinecone, FAISS) | Armazena embeddings de documentos para busca sem√¢ntica r√°pida. |
| **Implanta√ß√£o** | Docker, GCP Cloud Run, Firebase Hosting | Servi√ßos containerizados implantados em um ambiente sem servidor para escalabilidade. |
| **DevOps** | GitHub Actions, Semantic Release | CI/CD totalmente automatizado para versionamento e implanta√ß√£o. |

---

## üõ†Ô∏è Come√ßando: Desenvolvimento Local

A aplica√ß√£o √© containerizada para uma configura√ß√£o f√°cil.

### Pr√©-requisitos
* Docker & Docker Compose
* Git
* Chaves de API para o Google AI e/ou Groq.

### 1. Clone o Reposit√≥rio
```bash
git clone https://github.com/CidQueiroz/SenseiDB-Agent.git
cd SenseiDB-Agent
```

### 2. Configure as Vari√°veis de Ambiente

Crie um arquivo `.env.deploy` na raiz do projeto.

**Principais vari√°veis a serem configuradas:**
-   `GCP_PROJECT_ID`: Seu ID de Projeto do Google Cloud.
-   `GOOGLE_API_KEY`: Sua chave de API para o Gemini.
-   `GROQ_API_KEY`: Sua chave de API para o Groq.
-   `SECRET_KEY`: Uma chave secreta para o Django.

### 3. Construa e Execute a Aplica√ß√£o

```bash
docker-compose up --build
```
Este comando iniciar√° o backend Django e o frontend React.

-   **API do Backend** estar√° dispon√≠vel em `http://localhost:8000`.
-   **Aplica√ß√£o Frontend** estar√° dispon√≠vel em `http://localhost:8501`.

---

## üöÄ Pipeline de CI/CD

O projeto possui um pipeline de CI/CD totalmente automatizado usando GitHub Actions:

1.  **No Push para a `main`:**
    - Um workflow de `release` valida o c√≥digo e usa o `semantic-release` para criar uma tag de vers√£o com base nas mensagens de commit.
2.  **Em um Novo Release:**
    - Um workflow de `deploy` √© acionado.
    - Ele constr√≥i e envia uma imagem Docker do backend para o **Google Artifact Registry**.
    - Implanta esta nova imagem no **GCP Cloud Run**.
    - Constr√≥i e implanta o frontend React no **Firebase Hosting**.
