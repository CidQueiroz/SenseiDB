import os
from typing import List, Dict, Optional, Tuple
import firebase_admin
from firebase_admin import credentials, firestore
import google.generativeai as genai
from groq import Groq
import numpy as np
import requests

# ============================================
# CONFIGURA√á√ÉO
# ============================================

# Configura√ß√£o Firebase (singleton)
_firebase_initialized = False
_db = None

def init_firebase() -> Optional[firestore.Client]:
    """
    Inicializa Firebase Admin uma √∫nica vez
    
    Returns:
        Client do Firestore ou None se falhar
    """
    global _firebase_initialized, _db
    
    if not _firebase_initialized:
        try:
            cred = credentials.Certificate("firebase_credentials.json")
            firebase_admin.initialize_app(cred)
            _db = firestore.client()
            _firebase_initialized = True
            print("‚úÖ Firebase inicializado com sucesso")
        except Exception as e:
            print(f"‚ùå Erro ao inicializar Firebase: {e}")
            raise
    
    return _db


# Configura√ß√£o Google AI
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    print("‚ö†Ô∏è GOOGLE_API_KEY n√£o configurada")


# ============================================
# PROMPT SYSTEM GEN√âRICO
# ============================================

def gerar_prompt_sistema(contextos: List[str]) -> str:
    """
    Gera um prompt de sistema personalizado baseado nos contextos do usu√°rio
    
    Args:
        contextos: Lista de contextos/insights do usu√°rio
        
    Returns:
        Prompt de sistema formatado
    """
    
    # Prompt base gen√©rico
    prompt_base = """Voc√™ √© o "Sensei", um assistente de IA adaptativo e personalizado.

**SUA MISS√ÉO:**
Ajudar o usu√°rio a crescer, se desenvolver e alcan√ßar seus objetivos atrav√©s de:
- Autoconhecimento profundo
- Orienta√ß√£o estrat√©gica personalizada  
- Reflex√µes construtivas
- Planos de a√ß√£o pr√°ticos

**SEU M√âTODO:**
1. Analise o contexto e hist√≥rico do usu√°rio
2. Identifique padr√µes, desafios e oportunidades
3. Ofere√ßa insights relevantes e acion√°veis
4. Proponha pr√≥ximos passos concretos
5. Adapte sua abordagem ao perfil √∫nico da pessoa

**PRINC√çPIOS:**
- Seja emp√°tico mas direto
- Foque em a√ß√µes pr√°ticas, n√£o apenas teoria
- Reconhe√ßa conquistas e progressos
- Desafie cren√ßas limitantes de forma construtiva
- Mantenha uma vis√£o hol√≠stica (mente, corpo, rela√ß√µes, carreira)

**TOM DE VOZ:**
- S√°bio mas acess√≠vel
- Encorajador mas realista
- Questionador mas respeitoso
- Profundo mas pr√°tico"""

    # Se houver contextos, adiciona se√ß√£o personalizada
    if contextos:
        contexto_formatado = "\n".join([f"‚Ä¢ {ctx}" for ctx in contextos])
        
        prompt_personalizado = f"""

**CONTEXTO DO USU√ÅRIO:**
Voc√™ tem acesso aos seguintes insights e informa√ß√µes sobre este usu√°rio:

{contexto_formatado}

**IMPORTANTE:**
Use essas informa√ß√µes para personalizar completamente sua resposta:
- Mencione detalhes espec√≠ficos quando relevante
- Conecte a pergunta atual com o hist√≥rico dele
- Adapte exemplos e sugest√µes ao perfil dele
- Demonstre que voc√™ realmente conhece e acompanha a jornada dele
"""
        return prompt_base + prompt_personalizado
    
    else:
        prompt_sem_contexto = """

**OBSERVA√á√ÉO:**
Este usu√°rio ainda n√£o adicionou contextos pessoais. Suas respostas devem:
- Ser √∫teis de forma gen√©rica
- Encorajar o usu√°rio a compartilhar mais sobre si
- Explicar como contextos melhoram a personaliza√ß√£o
"""
        return prompt_base + prompt_sem_contexto


# ============================================
# FUN√á√ïES DE EMBEDDING E BUSCA
# ============================================

def buscar_contextos_relevantes(
    user_id: str, 
    query: str, 
    top_k: int = 5
) -> List[str]:
    """
    Busca contextos mais relevantes usando embeddings
    """
    try:
        db = init_firebase()
        
        # Gera embedding da query usando REST API (mais est√°vel)
        import requests
        
        api_key = os.environ.get('GOOGLE_API_KEY')
        if not api_key:
            print("‚ùå GOOGLE_API_KEY n√£o encontrada")
            return []
        
        # Usa REST API ao inv√©s de gRPC
        url = f"https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:embedContent?key={api_key}"
        
        payload = {
            "model": "models/text-embedding-004",
            "content": {
                "parts": [{"text": query}]
            }
        }
        
        response = requests.post(url, json=payload, timeout=30)
        
        if response.status_code != 200:
            print(f"‚ùå Erro na API: {response.status_code} - {response.text}")
            return []
        
        query_embedding = response.json()['embedding']['values']
        
        # Busca documentos do usu√°rio (apenas os que t√™m embedding)
        docs = db.collection('users').document(user_id).collection('inteligencia_critica').stream()
        
        contexts = []
        for doc in docs:
            doc_data = doc.to_dict()
            if 'embedding' in doc_data and 'contexto' in doc_data:
                doc_embedding = doc_data['embedding']
                
                # Similaridade de cosseno
                similarity = np.dot(query_embedding, doc_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
                )
                
                contexts.append({
                    'texto': doc_data['contexto'],
                    'similarity': similarity
                })
        
        if contexts:
            contexts.sort(key=lambda x: x['similarity'], reverse=True)
            return [ctx['texto'] for ctx in contexts[:top_k]]
        
        return []
    
    except Exception as e:
        print(f"‚ùå Erro ao buscar contextos: {e}")
        return []


def salvar_contexto_usuario(user_id: str, contexto_texto: str) -> bool:
    """Salva novo contexto com embedding usando REST API"""
    try:
        db = init_firebase()
        
        api_key = os.environ.get('GOOGLE_API_KEY')
        if not api_key:
            print("‚ùå GOOGLE_API_KEY n√£o encontrada")
            return False
        
        # Usa REST API
        url = f"https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:embedContent?key={api_key}"
        
        payload = {
            "model": "models/text-embedding-004",
            "content": {
                "parts": [{"text": contexto_texto}]
            }
        }
        
        response = requests.post(url, json=payload, timeout=30)
        
        if response.status_code != 200:
            print(f"‚ùå Erro na API: {response.status_code}")
            return False
        
        embedding = response.json()['embedding']['values']
        
        # Salva no Firestore
        user_doc_ref = db.collection('users').document(user_id)
        user_doc_ref.collection('inteligencia_critica').add({
            'contexto': contexto_texto,
            'timestamp': firestore.SERVER_TIMESTAMP,
            'embedding': embedding
        })
        
        print(f"‚úÖ Contexto salvo para user_id: {user_id}")
        return True
    
    except Exception as e:
        print(f"‚ùå Erro ao salvar contexto: {e}")
        return False


# ============================================
# FUN√á√ïES DE GERA√á√ÉO DE RESPOSTA
# ============================================

def gerar_resposta_groq(
    prompt: str, 
    api_key: str, 
    model: str = "llama-3.1-8b-instant"
) -> Tuple[Optional[str], Optional[str]]:
    """
    Gera resposta usando Groq
    
    Args:
        prompt: Prompt completo para a IA
        api_key: Chave API do Groq
        model: Modelo a usar
        
    Returns:
        Tupla (resposta, erro). Um dos dois ser√° None.
    """
    try:
        client = Groq(api_key=api_key)
        chat_completion = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=0.7,
            max_tokens=2048,
        )
        return chat_completion.choices[0].message.content, None
    except Exception as e:
        return None, str(e)


def gerar_resposta_google(
    prompt: str, 
    model_name: str = "gemini-2.0-flash"
) -> Tuple[Optional[str], Optional[str]]:
    """
    Gera resposta usando Google AI
    
    Args:
        prompt: Prompt completo para a IA
        model_name: Nome do modelo Gemini
        
    Returns:
        Tupla (resposta, erro). Um dos dois ser√° None.
    """
    try:
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        return response.text, None
    except Exception as e:
        return None, str(e)


# ============================================
# FUN√á√ÉO PRINCIPAL DE PROCESSAMENTO
# ============================================

def processar_query_usuario(
    user_id: str, 
    query: str, 
    groq_api_key: Optional[str] = None
) -> Dict[str, any]:
    """
    Processa a query do usu√°rio com RAG e IA
    
    Args:
        user_id: ID do usu√°rio
        query: Pergunta/mensagem do usu√°rio
        groq_api_key: Chave API do Groq (opcional)
        
    Returns:
        Dict com resposta, ia_usada e num_contextos
        
    Raises:
        Exception se ambas as IAs falharem
    """
    try:
        # 1. Busca contextos relevantes
        print(f"üîç Buscando contextos para user_id: {user_id}")
        contextos_relevantes = buscar_contextos_relevantes(user_id, query, top_k=5)
        
        if contextos_relevantes:
            print(f"‚úÖ {len(contextos_relevantes)} contextos encontrados")
        else:
            print("‚ö†Ô∏è Nenhum contexto encontrado")
        
        # 2. Gera prompt de sistema personalizado
        prompt_sistema = gerar_prompt_sistema(contextos_relevantes)
        
        # 3. Monta prompt final
        prompt_final = f"""{prompt_sistema}

---

**PERGUNTA/MENSAGEM DO USU√ÅRIO:**
{query}

---

**INSTRU√á√ïES FINAIS:**
- Responda em portugu√™s do Brasil
- Use markdown para formata√ß√£o (negrito, listas, etc)
- Seja conciso mas completo
- Termine com uma pergunta reflexiva ou pr√≥ximo passo claro
"""
        
        resposta_ia = None
        erro = None
        ia_usada = None
        
        # 4. Tenta Groq primeiro (se tiver chave)
        if groq_api_key:
            print("üîë Tentando Groq...")
            resposta_ia, erro = gerar_resposta_groq(prompt_final, groq_api_key)
            if resposta_ia:
                ia_usada = "groq"
        
        # 5. Fallback para Google AI
        if not resposta_ia:
            if erro:
                print(f"üîÑ Groq falhou ({erro}), usando Google AI")
            else:
                print("üåê Usando Google AI diretamente")
            
            resposta_ia, erro = gerar_resposta_google(prompt_final)
            if resposta_ia:
                ia_usada = "google"
        
        # 6. Verifica se conseguiu gerar resposta
        if not resposta_ia:
            raise Exception(f"Falha em ambas as IAs: {erro}")
        
        print(f"‚úÖ Resposta gerada com {ia_usada.upper()}")
        
        return {
            "resposta": resposta_ia,
            "ia_usada": ia_usada,
            "num_contextos": len(contextos_relevantes)
        }
    
    except Exception as e:
        print(f"‚ùå Erro ao processar query: {e}")
        raise